{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fatty-macintosh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdown_strings in /home/jules/anaconda/lib/python3.8/site-packages (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import dataclasses\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import transformers\n",
    "from typing import *\n",
    "\n",
    "!pip install markdown_strings\n",
    "import markdown_strings\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import rich\n",
    "import rich.console\n",
    "import rich.markdown\n",
    "import rich.table\n",
    "\n",
    "import tensorflow.python.framework.ops as ops\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.distribute.values as values\n",
    "import toolz\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "_PROJECT_DIRECTORY = pathlib.Path().resolve().parent\n",
    "sys.path.append(str(_PROJECT_DIRECTORY))\n",
    "import constants\n",
    "import task_specific\n",
    "import tf_utils\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parliamentary-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(text, escape=False):\n",
    "    if escape:\n",
    "        text = markdown_strings.esc_format(text)\n",
    "    display(Markdown(text))\n",
    "\n",
    "def h1(text, escape=False):\n",
    "    if escape:\n",
    "        text = markdown_strings.esc_format(text)\n",
    "    display(Markdown(f\"# {text}\"))\n",
    "    \n",
    "def h2(text, escape=False):\n",
    "    if escape:\n",
    "        text = markdown_strings.esc_format(text)\n",
    "    display(Markdown(f\"#### {text}\"))\n",
    "    \n",
    "def quote(text, escape=True):\n",
    "    if escape:\n",
    "        text = markdown_strings.esc_format(text)\n",
    "    display(Markdown(markdown_strings.blockquote(text)))\n",
    "    \n",
    "def build_split_to_ds_paths(project_directory, num_paths_display):\n",
    "    h1(\"Getting filenames.\")\n",
    "    h2(\"Loading json config.\")\n",
    "    config_path = project_directory/\"configs\"/\"train_configs\"/\"tpu_gpt2_eli5_kilt.json\"\n",
    "    config = utils.from_json_file(config_path)\n",
    "    \n",
    "    h2(\"Calling `gsutil ls` on the dataset repo.\")\n",
    "    ds_path = config[\"tfr_prefix\"]\n",
    "    filenames = subprocess.check_output(f\"gsutil ls {ds_path}\", shell=True).decode().strip().split(\"\\n\")\n",
    "\n",
    "    h2(\"Printing a few paths:\")\n",
    "    normal(f\"There are actually {len(filenames)}.\")\n",
    "    normal(\" - \" + \"\\n - \".join(filenames[:num_paths_display]))\n",
    "    \n",
    "    h1(\"Building the `per_split` Path dict.\")\n",
    "    per_split = collections.defaultdict(list)\n",
    "    for path in tqdm.tqdm(filenames, desc=\"Building `per_split` dict.\"):\n",
    "        split = pathlib.Path(path).name.split(\"_\")[0]\n",
    "        per_split[split].append(path)\n",
    "\n",
    "    normal(\"Sorting the `per_split` lists.\")\n",
    "    for split in per_split:\n",
    "        # Ad-hoc split per file index\n",
    "        per_split[split].sort(key=lambda p: int(pathlib.Path(p).name.split(\"_\")[1].split(\".\")[0]))\n",
    "\n",
    "    normal(\"Len per split for the per_split dict:\")\n",
    "    \n",
    "    print({split: len(per_split[split]) for split in per_split})\n",
    "    \n",
    "    return per_split\n",
    "\n",
    "\n",
    "def build_dataset(paths, context_window_size, split, batch_size):\n",
    "    ds = tf.data.TFRecordDataset(paths)\n",
    "    description = {\n",
    "      constants.CTH5Fields.distances:\n",
    "          tf.io.FixedLenFeature((), tf.string),\n",
    "      constants.CTH5Fields.gpt2_retrieved_ids:\n",
    "          tf.io.FixedLenFeature((), tf.string),\n",
    "      constants.CTH5Fields.gpt2_question_ids_inputs:\n",
    "          tf.io.FixedLenFeature((), tf.string),\n",
    "    }\n",
    "    if split != constants.SplitChoices.test:\n",
    "        description[\n",
    "            constants.CTH5Fields.gpt2_answer_ids_inputs\n",
    "        ] = tf.io.FixedLenFeature((), tf.string)\n",
    "\n",
    "    feature_dtypes = {\n",
    "      constants.CTH5Fields.distances:\n",
    "          tf.float32,\n",
    "      constants.CTH5Fields.gpt2_retrieved_ids:\n",
    "          tf.int32,\n",
    "      constants.CTH5Fields.gpt2_question_ids_inputs:\n",
    "          tf.int32,\n",
    "    }\n",
    "    if split != constants.SplitChoices.test:\n",
    "        feature_dtypes[\n",
    "            constants.CTH5Fields.gpt2_answer_ids_inputs\n",
    "        ] = tf.int32\n",
    "\n",
    "    feature_shape = {\n",
    "      constants.CTH5Fields.distances:\n",
    "          (10,),\n",
    "      constants.CTH5Fields.gpt2_retrieved_ids:\n",
    "          (10, context_window_size,),\n",
    "      constants.CTH5Fields.gpt2_question_ids_inputs:\n",
    "          (context_window_size,),\n",
    "    }\n",
    "    if split != constants.SplitChoices.test:\n",
    "        feature_shape[constants.CTH5Fields.gpt2_answer_ids_inputs] = (\n",
    "            context_window_size\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def parse(sample):\n",
    "        example = tf.io.parse_single_example(sample, description)\n",
    "        output = {}\n",
    "        for k, v in example.items():\n",
    "            output[k] = tf.io.parse_tensor(v, out_type=feature_dtypes[k])\n",
    "            output[k].set_shape(feature_shape[k])\n",
    "        return output\n",
    "\n",
    "    ds = ds.map(\n",
    "      parse,\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "      deterministic=False\n",
    "      )\n",
    "    \n",
    "    ds = ds.batch(\n",
    "      batch_size,\n",
    "      drop_remainder=split != constants.SplitChoices.test\n",
    "      )\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def decode_line(tokenizer, line):\n",
    "    return tokenizer.decode([x for x in line if x >= 0])\n",
    "\n",
    "\n",
    "def is_all_neg(tensor):\n",
    "    if not isinstance(tensor, (np.ndarray, tf.Tensor, ops.EagerTensor)):\n",
    "        return all([x < 0 for x in tensor])\n",
    "    else:\n",
    "        return np.all(tensor < 0)\n",
    "    \n",
    "    \n",
    "def check_and_decode(feature_key, item, tokenizer):\n",
    "    feature = item[feature_key]\n",
    "    all_neg = is_all_neg(feature)\n",
    "    assert not all_neg, feature_key\n",
    "    return decode_line(tokenizer, feature)\n",
    "    \n",
    "    \n",
    "def display_item(major, minor, tokenizer, item, split):    \n",
    "    item = vars(item)\n",
    "    ##################################################################################################################\n",
    "    # Produce information\n",
    "    ##################################################################################################################\n",
    "    question = check_and_decode(\n",
    "        constants.CTH5Fields.gpt2_question_ids_inputs,\n",
    "        item,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    answer = None\n",
    "    if split != \"test\":\n",
    "        feature_key = constants.CTH5Fields.gpt2_answer_ids_inputs\n",
    "        feature = item[feature_key]\n",
    "        answer = check_and_decode(feature_key, item, tokenizer)\n",
    "\n",
    "    retrieved_segments = []\n",
    "    for line in item[constants.CTH5Fields.gpt2_retrieved_ids]:\n",
    "        retrieved_segments.append(decode_line(tokenizer, line))\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # Display\n",
    "    ##################################################################################################################\n",
    "    console = rich.console.Console()\n",
    "    table = rich.table.Table(title=f\"{major}.{minor} - Item from split `{split}`\", show_lines=True)\n",
    "    table.add_column(\"Field\", style=\"bold\")\n",
    "    table.add_column(\"Value\")\n",
    "    table.add_row(\"Question:\", question)\n",
    "    for i, segment in enumerate(retrieved_segments):\n",
    "        table.add_row(f\"Retrieved segment {i}:\", segment)\n",
    "\n",
    "    if answer:\n",
    "        table.add_row(\"Answer:\", answer)\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "\n",
    "def check_all_unique(iterable):\n",
    "    \"\"\"Memory and computation scale in O(N) with N = len(iterable). \"\"\"\n",
    "    \n",
    "    iter_count = 0\n",
    "    set_ = set()\n",
    "    \n",
    "    for item in iterable:\n",
    "        iter_count += 1\n",
    "        set_.add(item) \n",
    "    \n",
    "    utils.check_equal(iter_count, len(set_))\n",
    "\n",
    "    \n",
    "@dataclasses.dataclass\n",
    "class Sample:\n",
    "    distances: tf.Tensor\n",
    "    gpt2_answer_ids: tf.Tensor\n",
    "    gpt2_question_ids: tf.Tensor\n",
    "    gpt2_retrieved_ids: tf.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handled-production",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: jules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: jules\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Getting filenames."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Loading json config."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Calling `gsutil ls` on the dataset repo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Printing a few paths:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are actually 8192."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_0.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_1.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_10.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_100.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_1000.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_1001.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_1002.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_1003.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_1004.tfr\n",
       " - gs://julesgm-research-v3/tfrecord_query_cache/20210225-191356/eval_1005.tfr"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Building the `per_split` Path dict."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6808dfe122964f79badff8cffbeb02fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building `per_split` dict.', max=8192.0, style=ProgressStâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sorting the `per_split` lists."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Len per split for the per_split dict:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval': 2048, 'test': 2048, 'train': 2048, 'validation': 2048}\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Long configuration stuff\n",
    "###############################################################################\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Flags\n",
    "#------------------------------------------------------------------------------\n",
    "_MAX_QTY = None\n",
    "_MODEL_TYPE = \"distilgpt2\"\n",
    "_EXPECTED_SIZES = dict(train=272634, eval=1507, test=600)\n",
    "_NUM_PATHS_DISPLAY = 10\n",
    "_NUM_REPLICAS = 8\n",
    "_ACCEL_TYPE = \"TPU\"\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# TPU Stuff\n",
    "#------------------------------------------------------------------------------\n",
    "if _ACCEL_TYPE == \"TPU\":\n",
    "    tpu_setup = tf_utils.init_tpus(socket.gethostname())\n",
    "    utils.check_equal(tf_utils.devices_to_use()[0].device_type, \"TPU\")\n",
    "    utils.check_equal(len(tf_utils.devices_to_use()), 8)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu_setup.resolver)\n",
    "elif _ACCEL_TYPE == \"CPU\":\n",
    "    device = tf_utils.devices_to_use()[0]\n",
    "    utils.check_equal(len(tf_utils.devices_to_use()), 1)\n",
    "    utils.check_equal(device.device_type, \"CPU\")\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device)\n",
    "    \n",
    "else:\n",
    "    raise RuntimeError(_ACCEL_TYPE)\n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------------------\n",
    "# Huggingface Stuff\n",
    "#------------------------------------------------------------------------------\n",
    "model_config = transformers.AutoConfig.from_pretrained(_MODEL_TYPE)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(_MODEL_TYPE)\n",
    "splits_to_ds_paths = build_split_to_ds_paths(_PROJECT_DIRECTORY, _NUM_PATHS_DISPLAY)\n",
    "context_window_size = model_config.n_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\n",
    "# \"eval\", \n",
    "# \"test\", \n",
    "    \"train\"\n",
    "]:\n",
    "    ds_paths = splits_to_ds_paths[split]\n",
    "    check_all_unique(ds_paths)\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Build and Distribute the DS\n",
    "    ###########################################################################\n",
    "    print(\"Building DS\")\n",
    "    ds = build_dataset(ds_paths, context_window_size, split, len(tf_utils.devices_to_use()))\n",
    "    ds = ds.batch(\n",
    "        len(tf_utils.devices_to_use()), drop_remainder=True\n",
    "    )\n",
    "    dds = strategy.experimental_distribute_dataset(ds)\n",
    "    print(\"Created DS\")\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Print elements of the DS\n",
    "    ###########################################################################\n",
    "    for major, dist_items in enumerate(toolz.take(2, dds)):\n",
    "        \n",
    "        turned_on_itself = [\n",
    "            Sample(None, None, None, None) for _ in range(len(tf_utils.devices_to_use()))\n",
    "        ]\n",
    "\n",
    "        for feature_key, v in dist_items.items():\n",
    "            if tf_utils.devices_to_use()[0].device_type == \"TPU\":\n",
    "                utils.check_isinstance(v, values.PerReplica)\n",
    "            else:\n",
    "                utils.check_isinstance(v, ops.EagerTensor)\n",
    "            \n",
    "            rich.print(len(v.values))\n",
    "            rich.print(v.values)\n",
    "            exit()\n",
    "            for i, vv in enumerate(v.values if isinstance(v, values.PerReplica) else [v]):\n",
    "                utils.check_equal(vv.shape[0], 1)\n",
    "                setattr(turned_on_itself[i], feature_key, vv[0])\n",
    "\n",
    "                \n",
    "        utils.check_equal(len(turned_on_itself), len(tf_utils.devices_to_use()))\n",
    "        for minor, item in enumerate(turned_on_itself):\n",
    "            rich.print(item)\n",
    "            if split == \"train\":\n",
    "                display_item(major, minor, tokenizer, item, split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-visibility",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
